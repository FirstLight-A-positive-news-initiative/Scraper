{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#BERT\n",
    "#Positivity Score\n",
    "from transformers import pipeline\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "#Summary\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#Capitalize the Sentences\n",
    "import textwrap\n",
    "import nltk.data\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedata=pd.read_csv(\"./LastRunDate.csv\")\n",
    "pastdate=datetime.fromtimestamp(datedata.loc[datedata[\"Scraper and Model\"]==\"ndtv\", \"Date\"])\n",
    "lastdate=pastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(executable_path=\"./chromedriver_win32/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchLinks(gen):    \n",
    "    driver.get(\"https://www.ndtv.com/\" + gen)\n",
    "    news_items = driver.find_elements_by_class_name(\"news_Itm\")\n",
    "    news_ads = driver.find_elements_by_class_name(\"adBg\")\n",
    "    all_news = []\n",
    "    news_items = list(set(news_items) ^ set(news_ads))\n",
    "    for news_item in news_items:\n",
    "        news_item_img = news_item.find_element_by_class_name(\"news_Itm-img\")\n",
    "        news_link = news_item_img.find_element_by_tag_name(\"a\")\n",
    "        \n",
    "        all_news.append(news_link.get_attribute(\"href\"))\n",
    "\n",
    "\n",
    "    # Moving to next pages:\n",
    "    req = requests.get(\"https://www.ndtv.com/\" + gen)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    pages = soup.find(class_ = \"listng_pagntn clear\")\n",
    "\n",
    "    for anchor in pages.findAll(\"a\")[1:10]:\n",
    "        req = requests.get(anchor.attrs[\"href\"])\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        news_link_img = soup.find_all(class_ = \"news_Itm-img\")\n",
    "        for news_link_img_itr in news_link_img:\n",
    "            all_news.append(news_link_img_itr.a.attrs[\"href\"])\n",
    "    return all_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_gathered = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_news(news_gathered, all_news, lastdate, gen):\n",
    "    for link in all_news:\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        soup = BeautifulSoup(requests.get(link).text, \"html.parser\")\n",
    "        \n",
    "        if(soup.find(class_ = \"sp-ttl\") and soup.find(class_ = \"sp-descp\") and soup.find(class_ = \"ins_instory_dv_cont\") and soup.find(class_ = \"ins_storybody\")):\n",
    "            heading = soup.find(class_ = \"sp-ttl\").text\n",
    "\n",
    "            description = soup.find(class_ = \"sp-descp\").text\n",
    "            \n",
    "            img_wrap = soup.find(class_ = \"ins_instory_dv_cont\")\n",
    "            image_link = \"\"\n",
    "            if(img_wrap.img):\n",
    "                image_link = img_wrap.img.attrs[\"src\"] \n",
    "            else:\n",
    "                image_link = \"\"\n",
    "\n",
    "            news_outerbody = soup.find(class_ = \"ins_storybody\")\n",
    "            content = \"\"\n",
    "            \n",
    "            paragraphs = news_outerbody.find_all(\"p\")\n",
    "            \n",
    "            date_wrap = soup.findAll(class_ = \"pst-by_li\")\n",
    "            if(date_wrap[-1].span.meta.attrs[\"content\"]):\n",
    "                date = datetime.strptime(date_wrap[-1].span.meta.attrs[\"content\"][0:-6], '%Y-%m-%dT%H:%M:%S')\n",
    "            else:\n",
    "                date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            if date<=pastdate:\n",
    "                continue\n",
    "            if lastdate<date:\n",
    "                lastdate=date\n",
    "\n",
    "            content = []\n",
    "            for para in paragraphs:\n",
    "                content.append(para.text)\n",
    "\n",
    "            content = \"\".join(content)\n",
    "            \n",
    "            news_gathered.append(\n",
    "                {\n",
    "                    \"title\": heading,\n",
    "                    \"link\": link,\n",
    "                    \"image_link\": image_link,\n",
    "                    \"summary\": content,\n",
    "                    \"desc\": description,\n",
    "                    \"positivity_score\": 0,\n",
    "                    \"date\": date,\n",
    "                    \"genre\": gen\n",
    "                }\n",
    "            )\n",
    "    return lastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastdate = gather_news(news_gathered, fetchLinks(\"world-news\"), lastdate, \"world\")\n",
    "lastdate = gather_news(news_gathered, fetchLinks(\"india\"), lastdate, \"india\")\n",
    "lastdate = gather_news(news_gathered, fetchLinks(\"science\"), lastdate, \"science\")\n",
    "lastdate = gather_news(news_gathered, fetchLinks(\"offbeat\"), lastdate, \"offbeat\")\n",
    "print(lastdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedata=pd.read_csv(\"./LastRunDate.csv\")\n",
    "datedata.loc[datedata[\"Scraper and Model\"]==\"ndtv\", \"Date\"]=datetime.timestamp(lastdate)\n",
    "datedata.to_csv(\"LastRunDate.csv\", index=False)\n",
    "print(\"Latest News Found:\", lastdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(news_gathered).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(news_gathered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_gathered = pd.DataFrame(news_gathered).dropna()\n",
    "news_gathered.drop_duplicates(keep='first', inplace=True)\n",
    "news_gathered.drop_duplicates(['title', 'genre'], keep='first', inplace=True)\n",
    "news_gathered = news_gathered.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_gathered)):\n",
    "    x={'label': 'NEGATIVE', 'score': 1}\n",
    "    try:\n",
    "        x = classifier(news_gathered[i][\"summary\"])[0]\n",
    "    except:\n",
    "        pass\n",
    "    z=classifier(news_gathered[i][\"desc\"])[0]\n",
    "    news_gathered[i][\"positivity_score\"] = max(x['score']/2+0.5 if x['label']=='POSITIVE' else (0.5-x['score']/2), z['score']/2+0.5 if z['label']=='POSITIVE' else (0.5-z['score']/2))\n",
    "    news_gathered[i][\"positivity_score\"] = int(float(news_gathered[i][\"positivity_score\"])*100)\n",
    "    if news_gathered[i][\"genre\"]==\"science\" or news_gathered[i][\"genre\"]==\"offbeat\":\n",
    "        news_gathered[i][\"positivity_score\"] += 50\n",
    "        news_gathered[i][\"positivity_score\"] = min(news_gathered[i][\"positivity_score\"], 100)\n",
    "pd.DataFrame(news_gathered).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes non-alphabetic characters:\n",
    "def text_strip(column):\n",
    "    for row in column:\n",
    "        \n",
    "        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n",
    "        \n",
    "        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n",
    "        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n",
    "        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
    "        \n",
    "        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n",
    "        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n",
    "        \n",
    "        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n",
    "        \n",
    "        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n",
    "        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n",
    "        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n",
    "        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n",
    "        \n",
    "        \n",
    "        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n",
    "        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n",
    "        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n",
    "        \n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "        \n",
    "        #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
    "        try:\n",
    "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
    "        except:\n",
    "            pass #there might be emails with no url in them\n",
    "        \n",
    "\n",
    "        \n",
    "        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n",
    "        \n",
    "        #Should always be last\n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "\n",
    "        row.strip().replace(\"\\n\",\"\")\n",
    "        \n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_gathered)):\n",
    "    content=news_gathered[i][\"summary\"]\n",
    "\n",
    "    preprocess_text = content.strip().replace(\"\\n\",\"\")\n",
    "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    # summmarize \n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                        num_beams=4,\n",
    "                                        no_repeat_ngram_size=2,\n",
    "                                        min_length=50,\n",
    "                                        max_length=200,\n",
    "                                        early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    sentences = sent_tokenizer.tokenize(output)\n",
    "    sentences = [sent.capitalize() for sent in sentences]\n",
    "    output=\"\"\n",
    "    for sent in sentences:\n",
    "        output+=sent+' '\n",
    "    news_gathered[i][\"summary\"]=output.strip()\n",
    "    print(\"Summarised:\", i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(news_gathered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_gathered = [{k: v for k, v in d.items() if k != 'desc'} for d in news_gathered]\n",
    "pd.DataFrame(news_gathered).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"\"\n",
    "password=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database():\n",
    "    # Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "    CONNECTION_STRING = f\"\"\n",
    "    try:\n",
    "        conn = MongoClient(CONNECTION_STRING, ssl_cert_reqs=ssl.CERT_NONE)\n",
    "        print(\"Connected successfully!!!\")\n",
    "        return conn.firstlight\n",
    "    except:  \n",
    "        print(\"Could not connect to MongoDB\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = get_database()\n",
    "try:\n",
    "    db[\"news\"].insert_many(news_gathered)\n",
    "    print(\"Success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
