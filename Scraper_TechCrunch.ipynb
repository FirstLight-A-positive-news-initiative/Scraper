{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#BERT\n",
    "from transformers import pipeline\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "import ssl\n",
    "\n",
    "#Summary\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#Capitalize the Sentences\n",
    "import textwrap\n",
    "import nltk.data\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import urllib.request, urllib.parse, urllib.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedata=pd.read_csv(\"./LastRunDate.csv\")\n",
    "pastdate=datetime.fromtimestamp(datedata.loc[datedata[\"Scraper and Model\"]==\"techcrunch\", \"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(executable_path=\"./chromedriver_win32/chromedriver\")\n",
    "driver.get(\"https://techcrunch.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    wait=WebDriverWait(driver, 100)\n",
    "    load=wait.until(EC.presence_of_element_located((By.XPATH, '//span[contains(@class, \"gradient-text gradient-text--green-gradient\")]')))\n",
    "    load.click()\n",
    "wait=WebDriverWait(driver, 100)\n",
    "load=wait.until(EC.presence_of_element_located((By.XPATH, '//span[contains(@class, \"gradient-text gradient-text--green-gradient\")]')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles=driver.find_elements_by_class_name('post-block__title')\n",
    "dates=[]\n",
    "print(\"News Scraped:\", len(titles))\n",
    "for i in range(len(titles)):\n",
    "    titles[i]=titles[i].find_element_by_class_name('post-block__title__link').get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastdate=pastdate\n",
    "news=[]\n",
    "for title in titles:\n",
    "#     Using Selenium\n",
    "    li=[]\n",
    "    driver.get(title)\n",
    "    \n",
    "    wait=WebDriverWait(driver, 100)\n",
    "    load=wait.until(EC.presence_of_element_located((By.XPATH, '//time[contains(@class, \"full-date-time\")]')))\n",
    "    utcdiff=datetime.now()-datetime.utcnow()\n",
    "    try:\n",
    "        newsdate=datetime.strptime(driver.find_element_by_class_name('full-date-time').get_attribute('datetime'), '%Y-%m-%dT%H:%M:%S')+utcdiff\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if newsdate<=pastdate:\n",
    "        continue\n",
    "\n",
    "    if lastdate<newsdate:\n",
    "        lastdate=newsdate\n",
    "    \n",
    "    try:\n",
    "        li.append(driver.find_element_by_class_name('article__title').text)\n",
    "        li.append(title)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        li.append(driver.find_element_by_class_name('article__featured-image').get_attribute('src'))\n",
    "    except:\n",
    "        li.append('')\n",
    "        \n",
    "    content=driver.find_element_by_class_name('article-content').find_elements_by_xpath(\"./*\")\n",
    "    li.append(content[0].text)\n",
    "    s=\"\"\n",
    "    for i in range(1, len(content)):\n",
    "        s=s+content[i].text+'\\n'\n",
    "    if len(s)==0:\n",
    "        continue\n",
    "    li.append(s)\n",
    "    li.append(0)\n",
    "    li.append(newsdate)\n",
    "    li.append('technology')    \n",
    "    \n",
    "    news.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedata=pd.read_csv(\"./LastRunDate.csv\")\n",
    "datedata.loc[datedata[\"Scraper and Model\"]==\"techcrunch\", \"Date\"]=datetime.timestamp(lastdate)\n",
    "datedata.to_csv(\"LastRunDate.csv\", index=False)\n",
    "print(\"Latest News Found:\", lastdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(news, columns=['title', 'link', 'image_link', 'summary', 'content', \"positivity_score\", 'date', 'genre'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes non-alphabetic characters:\n",
    "def text_strip(column):\n",
    "    for row in column:\n",
    "        \n",
    "        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n",
    "        \n",
    "        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n",
    "        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n",
    "        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
    "        \n",
    "        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n",
    "        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n",
    "        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n",
    "        \n",
    "        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n",
    "        \n",
    "        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n",
    "        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n",
    "        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n",
    "        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n",
    "        \n",
    "        \n",
    "        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n",
    "        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n",
    "        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n",
    "        \n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "        \n",
    "        #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
    "        try:\n",
    "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
    "        except:\n",
    "            pass #there might be emails with no url in them\n",
    "        \n",
    "\n",
    "        \n",
    "        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n",
    "        \n",
    "        #Should always be last\n",
    "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "\n",
    "        row.strip().replace(\"\\n\",\"\")\n",
    "        \n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "for i in range(len(X)):\n",
    "    content = X[i][4]\n",
    "\n",
    "    preprocess_text = content.strip().replace(\"\\n\",\"\")\n",
    "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "    # summmarize \n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                        num_beams=4,\n",
    "                                        no_repeat_ngram_size=2,\n",
    "                                        min_length=50,\n",
    "                                        max_length=200,\n",
    "                                        early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    sentences = sent_tokenizer.tokenize(output)\n",
    "    sentences = [sent.capitalize() for sent in sentences]\n",
    "    output=\"\"\n",
    "    for sent in sentences:\n",
    "        output+=sent+' '\n",
    "    X[i][3]=output.strip()\n",
    "    print(\"Summarised:\", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news_ in X:\n",
    "    y={'label': 'NEGATIVE', 'score': 1}\n",
    "    try:\n",
    "        y = classifier(news_[4][:2000])[0]\n",
    "    except:\n",
    "        pass\n",
    "    z = classifier(news_[3])[0]\n",
    "    news_[5] = max(y['score']/2+0.5 if y['label']=='POSITIVE' else (0.5-y['score']/2), z['score']/2+0.5 if z['label']=='POSITIVE' else (0.5-z['score']/2)) \n",
    "    news_[5] = np.int(np.float(news_[5])*100) + 25\n",
    "    news_[5] = min(news_[5], 100)\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(X, columns=['title', 'link', 'image_link', 'summary', 'content', \"positivity_score\", 'date', 'genre'])\n",
    "dataset=dataset.drop([\"content\"], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"\"\n",
    "password=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database():\n",
    "    # Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "    CONNECTION_STRING = f\"\"\n",
    "    try:\n",
    "        conn = MongoClient(CONNECTION_STRING, ssl_cert_reqs=ssl.CERT_NONE)\n",
    "        print(\"Connected successfully!!!\")\n",
    "        return conn.firstlight\n",
    "    except:  \n",
    "        print(\"Could not connect to MongoDB\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata = dataset.to_dict('records')\n",
    "print(\"News Filtered: \", len(newsdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_database()\n",
    "try:\n",
    "    db[\"news\"].insert_many(newsdata)\n",
    "    print(\"Success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db['news'].create_index(\"date\", expireAfterSeconds=31*24*60*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
